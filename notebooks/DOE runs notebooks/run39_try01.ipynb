{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aaebb4-037c-4ca4-a4c7-b63f03b071b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe0da97-852e-4243-a988-848082ac2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default initial values of DOE factors:\n",
    "# learning_rate = 0.001\n",
    "# dropout_value = 0.3\n",
    "# #n-conv_layers = 3\n",
    "# n_units_last_layer = 2048\n",
    "# n_filters_l1 = 32\n",
    "# n_filters_l2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e168a015-372f-4ba6-be82-501458ed9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOE factors:\n",
    "learning_rate = 0.0005\n",
    "dropout_value = 0.2\n",
    "# n-conv_layers = 4\n",
    "n_units_last_layer = 4096\n",
    "n_filters_l1 = 8\n",
    "n_filters_l2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f023191-b81d-488e-bcf7-47cb6de0ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other factors:\n",
    "img_size = 130\n",
    "batch_size = 32\n",
    "validation_split = 0.1  # 10% for validation\n",
    "test_split = 0.00  # 0% for testing\n",
    "shuffle_buffer_size = 1000\n",
    "seed_num = 101\n",
    "desired_accuracy = 0.99  # it should be active if EarlyStoppingCallback is activated\n",
    "loss = 'binary_crossentropy'\n",
    "#optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "metrics = ['accuracy']\n",
    "epochs = 15\n",
    "f_mode = 'nearest'  # fill_mode in image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4c91d-2f8f-4aad-8f3d-b5c4083437ab",
   "metadata": {},
   "source": [
    "    My dataset_root/\n",
    "    ├── woman/\n",
    "    │   ├── woman_1.jpg\n",
    "    │   ├── woman_2.jpg\n",
    "    │   ├── ...\n",
    "    ├── man/\n",
    "    │   ├── man_1.jpg\n",
    "    │   ├── man_2.jpg\n",
    "    │   ├── ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63419a3-8c4c-420c-a1ee-81d06c316f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 471 images of woman.\n",
      "There are 472 images of man.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"D:\\\\CS online courses\\\\Free DataSets\\\\Free Images\\\\Easier portrait images_GPU_03\"\n",
    "\n",
    "# Subdirectories for each class\n",
    "data_dir_woman = os.path.join(DATA_DIR, 'woman')\n",
    "data_dir_man = os.path.join(DATA_DIR, 'man')\n",
    "\n",
    "# os.listdir returns a list containing all files under the given dir\n",
    "print(f\"There are {len(os.listdir(data_dir_woman))} images of woman.\")\n",
    "print(f\"There are {len(os.listdir(data_dir_man))} images of man.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0d2806-2487-4ac5-97c4-ff2210416dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 943 files belonging to 2 classes.\n",
      "Using 849 files for training.\n",
      "Found 943 files belonging to 2 classes.\n",
      "Using 94 files for validation.\n",
      "Train batches: 27\n",
      "Validation batches: 3\n",
      "Test batches: 0\n"
     ]
    }
   ],
   "source": [
    "image_size = (img_size, img_size)  # Resize images to this size\n",
    "\n",
    "# Load train dataset (excluding validation & test set):\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = DATA_DIR,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split = validation_split + test_split,  # Total split for val + test\n",
    "    subset = \"training\",\n",
    "    seed = seed_num\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = DATA_DIR,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split = validation_split + test_split,\n",
    "    subset = \"validation\",\n",
    "    seed = seed_num\n",
    ")\n",
    "\n",
    "# Further manually split validation dataset to extract test dataset\n",
    "val_batches = tf.data.experimental.cardinality(val_dataset)\n",
    "# Compute test dataset size (number of batches)\n",
    "test_size = round(val_batches.numpy() * (test_split / (validation_split + test_split)))\n",
    "# Split validation dataset into validation and test subsets\n",
    "test_dataset = val_dataset.take(test_size)\n",
    "val_dataset = val_dataset.skip(test_size)\n",
    "\n",
    "\n",
    "print(f\"Train batches: {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
    "print(f\"Validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
    "print(f\"Test batches: {tf.data.experimental.cardinality(test_dataset).numpy()}\")\n",
    "\n",
    "# Optimize for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "training_dataset = train_dataset.cache().shuffle(shuffle_buffer_size).prefetch(buffer_size = AUTOTUNE)\n",
    "validation_dataset = val_dataset.cache().prefetch(buffer_size = AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d9e533-9ca2-45af-af85-a7e030fa184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum pixel value of images: 255.0\n",
      "\n",
      "Shape of batch of images: (32, 130, 130, 3)\n",
      "Shape of batch of labels: (32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch of images and labels\n",
    "for images, labels in training_dataset.take(1):\n",
    "\texample_batch_images = images\n",
    "\texample_batch_labels = labels\n",
    "\n",
    "max_pixel = np.max(example_batch_images)\n",
    "print(f\"Maximum pixel value of images: {max_pixel}\\n\")\n",
    "print(f\"Shape of batch of images: {example_batch_images.shape}\")\n",
    "print(f\"Shape of batch of labels: {example_batch_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85453247-adf0-4e7d-8924-f86415759f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass EarlyStoppingCallback(tf.keras.callbacks.Callback):\\n    def on_epoch_end(self, epoch, logs=None):\\n        train_accuracy = logs.get(\\'accuracy\\')\\n        val_accuracy = logs.get(\\'val_accuracy\\')\\n        if train_accuracy >= desired_accuracy and val_accuracy >= desired_accuracy:\\n            self.model.stop_training = True\\n            print(f\"\\nReached {desired_accuracy}% accuracy so cancelling training!\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_accuracy = logs.get('accuracy')\n",
    "        val_accuracy = logs.get('val_accuracy')\n",
    "        if train_accuracy >= desired_accuracy and val_accuracy >= desired_accuracy:\n",
    "            self.model.stop_training = True\n",
    "            print(f\"\\nReached {desired_accuracy}% accuracy so cancelling training!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7906f3e8-d63c-4210-b096-5ee54e2c1895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nearly_stop = EarlyStopping(monitor='val_loss', patience=3)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7fe37b-3831-478d-9fd5-d018ce786472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Reduce LR every 10 epochs (Learning rate decay factor)\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        return lr * 1.0\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06db003d-e408-472a-986b-54e0728db183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation_model\n",
    "def augment_model():\n",
    "    \"\"\"Creates a model (layers stacked on top of each other) for augmenting images of woman and man.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The model made up of the layers that will be used to augment the images of woman and man.\n",
    "    \"\"\"\n",
    "\n",
    "    augmentation_model = tf.keras.Sequential([\n",
    "        # Specify the input shape.\n",
    "        tf.keras.Input(shape = (img_size, img_size, 3)),\n",
    "        \n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1, fill_mode = f_mode),\n",
    "        #tf.keras.layers.RandomTranslation(0.1, 0.1, fill_mode = f_mode),\n",
    "        #tf.keras.layers.RandomZoom(0.1, fill_mode=f_mode)\n",
    "        ])\n",
    "\n",
    "    return augmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d8ea70-8df3-4dd5-93e0-013cd42546e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compile_model():\n",
    "    \"\"\"Creates, compiles and trains the model to predict woman and man images.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The model that will be trained to predict woman and man images.\n",
    "    \"\"\"\n",
    "\n",
    "    augmentation_layers = augment_model()\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # Note: the input shape is the desired size of the image: 150x150 with 3 bytes for color\n",
    "        tf.keras.layers.InputLayer(shape = (img_size, img_size, 3)),\n",
    "        augmentation_layers,\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        #####    CONV_LAYER_1:     #####\n",
    "        tf.keras.layers.Conv2D(n_filters_l1, (4, 4), activation = 'linear'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        #####    CONV_LAYER_2:     #####\n",
    "        tf.keras.layers.Conv2D(n_filters_l2, (3, 3), activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        #####    CONV_LAYER_3:     #####\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        #####    CONV_LAYER_4:     #####\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(dropout_value),\n",
    "        #####    BEFORE_LAST_LAYER:     #####\n",
    "        tf.keras.layers.Dense(n_units_last_layer, activation = 'relu'),\n",
    "        # It will contain a value from 0-1 where 0 for the class 'female' and 1 for the 'male'\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
    "\n",
    "    model.compile(\n",
    "        loss = loss,\n",
    "        optimizer = optimizer,\n",
    "        metrics = metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ecc4042-af60-4d6a-a989-f81761b5bc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,441,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,097</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │           \u001b[38;5;34m392\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m4,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │     \u001b[38;5;34m9,441,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m4,097\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,524,297</span> (36.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,524,297\u001b[0m (36.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,524,297</span> (36.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,524,297\u001b[0m (36.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the compiled but untrained model\n",
    "model = create_and_compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89abb112-31e4-462d-aeb5-ac878eb84cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_history = model.fit(\\n    training_dataset,\\n    epochs = epochs,\\n    validation_data = validation_dataset,\\n    callbacks = [EarlyStoppingCallback()],\\n    verbose = 2\\n)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks = [EarlyStoppingCallback()],\n",
    "    verbose = 2\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b5a348-8f19-43d7-b96d-764b239959e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_history = model.fit(\\n    training_dataset,\\n    epochs = epochs,\\n    validation_data = validation_dataset,\\n    callbacks=[early_stop],\\n    verbose = 2\\n)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks=[early_stop],\n",
    "    verbose = 2\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e9ceea5-9f80-4011-aeb4-9951bc88ac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 - 4s - 156ms/step - accuracy: 0.5654 - loss: 0.6943 - val_accuracy: 0.5851 - val_loss: 0.6787 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "27/27 - 2s - 83ms/step - accuracy: 0.6537 - loss: 0.6067 - val_accuracy: 0.7447 - val_loss: 0.5813 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "27/27 - 2s - 88ms/step - accuracy: 0.7362 - loss: 0.5293 - val_accuracy: 0.8191 - val_loss: 0.4940 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "27/27 - 2s - 85ms/step - accuracy: 0.7739 - loss: 0.4935 - val_accuracy: 0.7979 - val_loss: 0.4737 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "27/27 - 2s - 83ms/step - accuracy: 0.7809 - loss: 0.4596 - val_accuracy: 0.7979 - val_loss: 0.4935 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "27/27 - 2s - 83ms/step - accuracy: 0.7915 - loss: 0.4607 - val_accuracy: 0.7979 - val_loss: 0.4697 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "27/27 - 2s - 80ms/step - accuracy: 0.7703 - loss: 0.4746 - val_accuracy: 0.7447 - val_loss: 0.5364 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "27/27 - 2s - 79ms/step - accuracy: 0.7739 - loss: 0.4501 - val_accuracy: 0.7979 - val_loss: 0.5643 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      2\u001b[0m     training_dataset,\n\u001b[0;32m      3\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m epochs,\n\u001b[0;32m      4\u001b[0m     validation_data \u001b[38;5;241m=\u001b[39m validation_dataset,\n\u001b[0;32m      5\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [lr_callback],\n\u001b[0;32m      6\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks = [lr_callback],\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32643e-ba36-4ab6-bf3b-43313b52bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import load_model\n",
    "#model.save('gender_recognition_project04_v10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ce52e-7f7c-47c7-9d63-f8d21a4ef5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16344a-bbc7-4ed5-a01e-39b651f802c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history = pd.DataFrame(model.history.history)\n",
    "result_history.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8df04-5bbc-4d47-a131-1e4a3a1c7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history[['loss', 'val_loss']].plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b61e9e-deaf-4f91-8b90-d23082a18b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history[['accuracy', 'val_accuracy']].plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b259fcb-df40-4d21-a911-032373c25358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)\n",
    "print(model.evaluate(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce3add-a314-4d29-91ea-76cb3ba57cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = np.concatenate([y.numpy() for _, y in validation_dataset])\n",
    "y_pred_prob = model.predict(validation_dataset)\n",
    "# Convert probabilities to class labels (0:Female or 1:Male)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=['Female', 'Male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063526cc-e2a0-4c8c-b01c-47ae3654a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "img_size = img_size\n",
    "model = tf.keras.models.load_model(\"gender_recognition_project04_v10.h5\")\n",
    "\n",
    "# Load your personal image if you are interested to predict:\n",
    "your_image_path = \"D:\\\\Hossein's desktop files in Microsoft Studio Laptop\\\\Personal Photos\\\\Hossein_10.jpg\"\n",
    "\n",
    "img = load_img(your_image_path, target_size=(img_size, img_size))\n",
    "final_img = img_to_array(img)\n",
    "# Adding a batch dimension:\n",
    "final_img = np.expand_dims(final_img, axis=0)\n",
    "prediction = model.predict(final_img)\n",
    "result = \"Female\" if prediction > 0.5 else \"Male\"\n",
    "if result==\"Female\":\n",
    "    confidence = (model.predict(final_img)[0][0])*100\n",
    "else:\n",
    "    confidence = (1-model.predict(final_img)[0][0])*100\n",
    "print(f\"Prediction result: {result} (confidence= {confidence:.2f} %)\")\n",
    "\n",
    "# Visualize CNN Layers\n",
    "successive_feature_maps = visualization_model.predict(final_img)\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    if len(feature_map.shape) == 4:  # Only visualize conv/maxpool layers\n",
    "        n_features = feature_map.shape[-1]  # Number of filters\n",
    "        size = feature_map.shape[1]  # Feature map size\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "\n",
    "        for i in range(n_features):\n",
    "            x = feature_map[0, :, :, i]\n",
    "            x -= x.mean()\n",
    "            x /= (x.std() + 1e-8)  # Normalize\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')  # Convert to image format\n",
    "            display_grid[:, i * size: (i + 1) * size] = x\n",
    "\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='cividis')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199738b-2405-4223-9eae-21b5840224a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a87fb2-d828-420e-883f-709ca46ba4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb26553-bde6-4c23-ba84-69763691803d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
