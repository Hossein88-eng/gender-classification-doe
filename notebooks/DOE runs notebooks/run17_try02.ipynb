{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aaebb4-037c-4ca4-a4c7-b63f03b071b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe0da97-852e-4243-a988-848082ac2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default initial values of DOE factors:\n",
    "# learning_rate = 0.001\n",
    "# dropout_value = 0.3\n",
    "# #n-conv_layers = 3\n",
    "# n_units_last_layer = 2048\n",
    "# n_filters_l1 = 32\n",
    "# n_filters_l2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e168a015-372f-4ba6-be82-501458ed9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOE factors:\n",
    "learning_rate = 0.001\n",
    "dropout_value = 0.3\n",
    "# n-conv_layers = 3\n",
    "n_units_last_layer = 2048\n",
    "n_filters_l1 = 64\n",
    "n_filters_l2 = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f023191-b81d-488e-bcf7-47cb6de0ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other factors:\n",
    "img_size = 130\n",
    "batch_size = 32\n",
    "validation_split = 0.1  # 10% for validation\n",
    "test_split = 0.00  # 0% for testing\n",
    "shuffle_buffer_size = 1000\n",
    "seed_num = 101\n",
    "desired_accuracy = 0.99  # it should be active if EarlyStoppingCallback is activated\n",
    "loss = 'binary_crossentropy'\n",
    "#optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "metrics = ['accuracy']\n",
    "epochs = 15\n",
    "f_mode = 'nearest'  # fill_mode in image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4c91d-2f8f-4aad-8f3d-b5c4083437ab",
   "metadata": {},
   "source": [
    "    My dataset_root/\n",
    "    ├── woman/\n",
    "    │   ├── woman_1.jpg\n",
    "    │   ├── woman_2.jpg\n",
    "    │   ├── ...\n",
    "    ├── man/\n",
    "    │   ├── man_1.jpg\n",
    "    │   ├── man_2.jpg\n",
    "    │   ├── ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63419a3-8c4c-420c-a1ee-81d06c316f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 471 images of woman.\n",
      "There are 472 images of man.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"D:\\\\CS online courses\\\\Free DataSets\\\\Free Images\\\\Easier portrait images_GPU_03\"\n",
    "\n",
    "# Subdirectories for each class\n",
    "data_dir_woman = os.path.join(DATA_DIR, 'woman')\n",
    "data_dir_man = os.path.join(DATA_DIR, 'man')\n",
    "\n",
    "# os.listdir returns a list containing all files under the given dir\n",
    "print(f\"There are {len(os.listdir(data_dir_woman))} images of woman.\")\n",
    "print(f\"There are {len(os.listdir(data_dir_man))} images of man.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0d2806-2487-4ac5-97c4-ff2210416dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 943 files belonging to 2 classes.\n",
      "Using 849 files for training.\n",
      "Found 943 files belonging to 2 classes.\n",
      "Using 94 files for validation.\n",
      "Train batches: 27\n",
      "Validation batches: 3\n",
      "Test batches: 0\n"
     ]
    }
   ],
   "source": [
    "image_size = (img_size, img_size)  # Resize images to this size\n",
    "\n",
    "# Load train dataset (excluding validation & test set):\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = DATA_DIR,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split = validation_split + test_split,  # Total split for val + test\n",
    "    subset = \"training\",\n",
    "    seed = seed_num\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = DATA_DIR,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split = validation_split + test_split,\n",
    "    subset = \"validation\",\n",
    "    seed = seed_num\n",
    ")\n",
    "\n",
    "# Further manually split validation dataset to extract test dataset\n",
    "val_batches = tf.data.experimental.cardinality(val_dataset)\n",
    "# Compute test dataset size (number of batches)\n",
    "test_size = round(val_batches.numpy() * (test_split / (validation_split + test_split)))\n",
    "# Split validation dataset into validation and test subsets\n",
    "test_dataset = val_dataset.take(test_size)\n",
    "val_dataset = val_dataset.skip(test_size)\n",
    "\n",
    "\n",
    "print(f\"Train batches: {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
    "print(f\"Validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
    "print(f\"Test batches: {tf.data.experimental.cardinality(test_dataset).numpy()}\")\n",
    "\n",
    "# Optimize for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "training_dataset = train_dataset.cache().shuffle(shuffle_buffer_size).prefetch(buffer_size = AUTOTUNE)\n",
    "validation_dataset = val_dataset.cache().prefetch(buffer_size = AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d9e533-9ca2-45af-af85-a7e030fa184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum pixel value of images: 255.0\n",
      "\n",
      "Shape of batch of images: (32, 130, 130, 3)\n",
      "Shape of batch of labels: (32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch of images and labels\n",
    "for images, labels in training_dataset.take(1):\n",
    "\texample_batch_images = images\n",
    "\texample_batch_labels = labels\n",
    "\n",
    "max_pixel = np.max(example_batch_images)\n",
    "print(f\"Maximum pixel value of images: {max_pixel}\\n\")\n",
    "print(f\"Shape of batch of images: {example_batch_images.shape}\")\n",
    "print(f\"Shape of batch of labels: {example_batch_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85453247-adf0-4e7d-8924-f86415759f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass EarlyStoppingCallback(tf.keras.callbacks.Callback):\\n    def on_epoch_end(self, epoch, logs=None):\\n        train_accuracy = logs.get(\\'accuracy\\')\\n        val_accuracy = logs.get(\\'val_accuracy\\')\\n        if train_accuracy >= desired_accuracy and val_accuracy >= desired_accuracy:\\n            self.model.stop_training = True\\n            print(f\"\\nReached {desired_accuracy}% accuracy so cancelling training!\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_accuracy = logs.get('accuracy')\n",
    "        val_accuracy = logs.get('val_accuracy')\n",
    "        if train_accuracy >= desired_accuracy and val_accuracy >= desired_accuracy:\n",
    "            self.model.stop_training = True\n",
    "            print(f\"\\nReached {desired_accuracy}% accuracy so cancelling training!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7906f3e8-d63c-4210-b096-5ee54e2c1895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nearly_stop = EarlyStopping(monitor='val_loss', patience=3)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7fe37b-3831-478d-9fd5-d018ce786472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Reduce LR every 10 epochs (Learning rate decay factor)\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        return lr * 1.0\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06db003d-e408-472a-986b-54e0728db183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation_model\n",
    "def augment_model():\n",
    "    \"\"\"Creates a model (layers stacked on top of each other) for augmenting images of woman and man.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The model made up of the layers that will be used to augment the images of woman and man.\n",
    "    \"\"\"\n",
    "\n",
    "    augmentation_model = tf.keras.Sequential([\n",
    "        # Specify the input shape.\n",
    "        tf.keras.Input(shape = (img_size, img_size, 3)),\n",
    "        \n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1, fill_mode = f_mode),\n",
    "        #tf.keras.layers.RandomTranslation(0.1, 0.1, fill_mode = f_mode),\n",
    "        #tf.keras.layers.RandomZoom(0.1, fill_mode=f_mode)\n",
    "        ])\n",
    "\n",
    "    return augmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d8ea70-8df3-4dd5-93e0-013cd42546e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compile_model():\n",
    "    \"\"\"Creates, compiles and trains the model to predict woman and man images.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The model that will be trained to predict woman and man images.\n",
    "    \"\"\"\n",
    "\n",
    "    augmentation_layers = augment_model()\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # Note: the input shape is the desired size of the image: 150x150 with 3 bytes for color\n",
    "        tf.keras.layers.InputLayer(shape = (img_size, img_size, 3)),\n",
    "        augmentation_layers,\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        #####    CONV_LAYER_1:     #####\n",
    "        tf.keras.layers.Conv2D(n_filters_l1, (4, 4), activation = 'linear'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        #####    CONV_LAYER_2:     #####\n",
    "        tf.keras.layers.Conv2D(n_filters_l2, (3, 3), activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        #####    CONV_LAYER_3:     #####\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(dropout_value),\n",
    "        #####    BEFORE_LAST_LAYER:     #####\n",
    "        tf.keras.layers.Dense(n_units_last_layer, activation = 'relu'),\n",
    "        # It will contain a value from 0-1 where 0 for the class 'female' and 1 for the 'male'\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
    "\n",
    "    model.compile(\n",
    "        loss = loss,\n",
    "        optimizer = optimizer,\n",
    "        metrics = metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ecc4042-af60-4d6a-a989-f81761b5bc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,692,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │    \u001b[38;5;34m25,692,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m2,049\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,734,305</span> (98.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,734,305\u001b[0m (98.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,734,305</span> (98.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,734,305\u001b[0m (98.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the compiled but untrained model\n",
    "model = create_and_compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89abb112-31e4-462d-aeb5-ac878eb84cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_history = model.fit(\\n    training_dataset,\\n    epochs = epochs,\\n    validation_data = validation_dataset,\\n    callbacks = [EarlyStoppingCallback()],\\n    verbose = 2\\n)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks = [EarlyStoppingCallback()],\n",
    "    verbose = 2\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b5a348-8f19-43d7-b96d-764b239959e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_history = model.fit(\\n    training_dataset,\\n    epochs = epochs,\\n    validation_data = validation_dataset,\\n    callbacks=[early_stop],\\n    verbose = 2\\n)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks=[early_stop],\n",
    "    verbose = 2\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ceea5-9f80-4011-aeb4-9951bc88ac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "27/27 - 6s - 236ms/step - accuracy: 0.5548 - loss: 0.7946 - val_accuracy: 0.5957 - val_loss: 0.6420 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "27/27 - 5s - 167ms/step - accuracy: 0.6796 - loss: 0.5954 - val_accuracy: 0.5000 - val_loss: 0.9960 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "27/27 - 5s - 179ms/step - accuracy: 0.7279 - loss: 0.5459 - val_accuracy: 0.7340 - val_loss: 0.5382 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "27/27 - 4s - 162ms/step - accuracy: 0.7574 - loss: 0.4900 - val_accuracy: 0.7447 - val_loss: 0.4746 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "27/27 - 4s - 162ms/step - accuracy: 0.7680 - loss: 0.5036 - val_accuracy: 0.8191 - val_loss: 0.4708 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "27/27 - 5s - 169ms/step - accuracy: 0.7998 - loss: 0.4480 - val_accuracy: 0.8191 - val_loss: 0.4886 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "27/27 - 5s - 170ms/step - accuracy: 0.8009 - loss: 0.4244 - val_accuracy: 0.8298 - val_loss: 0.4616 - learning_rate: 0.0010\n",
      "Epoch 8/15\n"
     ]
    }
   ],
   "source": [
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks = [lr_callback],\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32643e-ba36-4ab6-bf3b-43313b52bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import load_model\n",
    "#model.save('gender_recognition_project04_v10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ce52e-7f7c-47c7-9d63-f8d21a4ef5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16344a-bbc7-4ed5-a01e-39b651f802c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history = pd.DataFrame(model.history.history)\n",
    "result_history.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8df04-5bbc-4d47-a131-1e4a3a1c7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history[['loss', 'val_loss']].plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b61e9e-deaf-4f91-8b90-d23082a18b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history[['accuracy', 'val_accuracy']].plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b259fcb-df40-4d21-a911-032373c25358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)\n",
    "print(model.evaluate(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce3add-a314-4d29-91ea-76cb3ba57cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = np.concatenate([y.numpy() for _, y in validation_dataset])\n",
    "y_pred_prob = model.predict(validation_dataset)\n",
    "# Convert probabilities to class labels (0:Female or 1:Male)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=['Female', 'Male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063526cc-e2a0-4c8c-b01c-47ae3654a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "img_size = img_size\n",
    "model = tf.keras.models.load_model(\"gender_recognition_project04_v10.h5\")\n",
    "\n",
    "# Load your personal image if you are interested to predict:\n",
    "your_image_path = \"D:\\\\Hossein's desktop files in Microsoft Studio Laptop\\\\Personal Photos\\\\Hossein_10.jpg\"\n",
    "\n",
    "img = load_img(your_image_path, target_size=(img_size, img_size))\n",
    "final_img = img_to_array(img)\n",
    "# Adding a batch dimension:\n",
    "final_img = np.expand_dims(final_img, axis=0)\n",
    "prediction = model.predict(final_img)\n",
    "result = \"Female\" if prediction > 0.5 else \"Male\"\n",
    "if result==\"Female\":\n",
    "    confidence = (model.predict(final_img)[0][0])*100\n",
    "else:\n",
    "    confidence = (1-model.predict(final_img)[0][0])*100\n",
    "print(f\"Prediction result: {result} (confidence= {confidence:.2f} %)\")\n",
    "\n",
    "# Visualize CNN Layers\n",
    "successive_feature_maps = visualization_model.predict(final_img)\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    if len(feature_map.shape) == 4:  # Only visualize conv/maxpool layers\n",
    "        n_features = feature_map.shape[-1]  # Number of filters\n",
    "        size = feature_map.shape[1]  # Feature map size\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "\n",
    "        for i in range(n_features):\n",
    "            x = feature_map[0, :, :, i]\n",
    "            x -= x.mean()\n",
    "            x /= (x.std() + 1e-8)  # Normalize\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')  # Convert to image format\n",
    "            display_grid[:, i * size: (i + 1) * size] = x\n",
    "\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='cividis')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199738b-2405-4223-9eae-21b5840224a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a87fb2-d828-420e-883f-709ca46ba4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb26553-bde6-4c23-ba84-69763691803d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
