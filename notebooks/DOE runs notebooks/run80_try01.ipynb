{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aaebb4-037c-4ca4-a4c7-b63f03b071b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe0da97-852e-4243-a988-848082ac2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default initial values of DOE factors:\n",
    "# learning_rate = 0.001\n",
    "# dropout_value = 0.3\n",
    "# #n-conv_layers = 3\n",
    "# n_units_last_layer = 2048\n",
    "# n_filters_l1 = 32\n",
    "# n_filters_l2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e168a015-372f-4ba6-be82-501458ed9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOE factors:\n",
    "learning_rate = 0.005\n",
    "dropout_value = 0.2\n",
    "# n-conv_layers = 4\n",
    "n_units_last_layer = 4096\n",
    "n_filters_l1 = 8\n",
    "n_filters_l2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f023191-b81d-488e-bcf7-47cb6de0ed76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     12\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# other factors:\n",
    "img_size = 130\n",
    "batch_size = 32\n",
    "validation_split = 0.1  # 10% for validation\n",
    "test_split = 0.00  # 0% for testing\n",
    "shuffle_buffer_size = 1000\n",
    "seed_num = 101\n",
    "desired_accuracy = 0.99  # it should be active if EarlyStoppingCallback is activated\n",
    "loss = 'binary_crossentropy'\n",
    "#optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "metrics = ['accuracy']\n",
    "epochs = 15\n",
    "f_mode = 'nearest'  # fill_mode in image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4c91d-2f8f-4aad-8f3d-b5c4083437ab",
   "metadata": {},
   "source": [
    "    My dataset_root/\n",
    "    ├── woman/\n",
    "    │   ├── woman_1.jpg\n",
    "    │   ├── woman_2.jpg\n",
    "    │   ├── ...\n",
    "    ├── man/\n",
    "    │   ├── man_1.jpg\n",
    "    │   ├── man_2.jpg\n",
    "    │   ├── ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63419a3-8c4c-420c-a1ee-81d06c316f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"D:\\\\CS online courses\\\\Free DataSets\\\\Free Images\\\\Easier portrait images_GPU_03\"\n",
    "\n",
    "# Subdirectories for each class\n",
    "data_dir_woman = os.path.join(DATA_DIR, 'woman')\n",
    "data_dir_man = os.path.join(DATA_DIR, 'man')\n",
    "\n",
    "# os.listdir returns a list containing all files under the given dir\n",
    "print(f\"There are {len(os.listdir(data_dir_woman))} images of woman.\")\n",
    "print(f\"There are {len(os.listdir(data_dir_man))} images of man.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d2806-2487-4ac5-97c4-ff2210416dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (img_size, img_size)  # Resize images to this size\n",
    "\n",
    "# Load train dataset (excluding validation & test set):\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = DATA_DIR,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split = validation_split + test_split,  # Total split for val + test\n",
    "    subset = \"training\",\n",
    "    seed = seed_num\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = DATA_DIR,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    label_mode='binary',\n",
    "    validation_split = validation_split + test_split,\n",
    "    subset = \"validation\",\n",
    "    seed = seed_num\n",
    ")\n",
    "\n",
    "# Further manually split validation dataset to extract test dataset\n",
    "val_batches = tf.data.experimental.cardinality(val_dataset)\n",
    "# Compute test dataset size (number of batches)\n",
    "test_size = round(val_batches.numpy() * (test_split / (validation_split + test_split)))\n",
    "# Split validation dataset into validation and test subsets\n",
    "test_dataset = val_dataset.take(test_size)\n",
    "val_dataset = val_dataset.skip(test_size)\n",
    "\n",
    "\n",
    "print(f\"Train batches: {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
    "print(f\"Validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
    "print(f\"Test batches: {tf.data.experimental.cardinality(test_dataset).numpy()}\")\n",
    "\n",
    "# Optimize for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "training_dataset = train_dataset.cache().shuffle(shuffle_buffer_size).prefetch(buffer_size = AUTOTUNE)\n",
    "validation_dataset = val_dataset.cache().prefetch(buffer_size = AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9e533-9ca2-45af-af85-a7e030fa184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch of images and labels\n",
    "for images, labels in training_dataset.take(1):\n",
    "\texample_batch_images = images\n",
    "\texample_batch_labels = labels\n",
    "\n",
    "max_pixel = np.max(example_batch_images)\n",
    "print(f\"Maximum pixel value of images: {max_pixel}\\n\")\n",
    "print(f\"Shape of batch of images: {example_batch_images.shape}\")\n",
    "print(f\"Shape of batch of labels: {example_batch_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85453247-adf0-4e7d-8924-f86415759f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_accuracy = logs.get('accuracy')\n",
    "        val_accuracy = logs.get('val_accuracy')\n",
    "        if train_accuracy >= desired_accuracy and val_accuracy >= desired_accuracy:\n",
    "            self.model.stop_training = True\n",
    "            print(f\"\\nReached {desired_accuracy}% accuracy so cancelling training!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906f3e8-d63c-4210-b096-5ee54e2c1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7fe37b-3831-478d-9fd5-d018ce786472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Reduce LR every 10 epochs (Learning rate decay factor)\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        return lr * 1.0\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db003d-e408-472a-986b-54e0728db183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation_model\n",
    "def augment_model():\n",
    "    \"\"\"Creates a model (layers stacked on top of each other) for augmenting images of woman and man.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The model made up of the layers that will be used to augment the images of woman and man.\n",
    "    \"\"\"\n",
    "\n",
    "    augmentation_model = tf.keras.Sequential([\n",
    "        # Specify the input shape.\n",
    "        tf.keras.Input(shape = (img_size, img_size, 3)),\n",
    "        \n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1, fill_mode = f_mode),\n",
    "        #tf.keras.layers.RandomTranslation(0.1, 0.1, fill_mode = f_mode),\n",
    "        #tf.keras.layers.RandomZoom(0.1, fill_mode=f_mode)\n",
    "        ])\n",
    "\n",
    "    return augmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8ea70-8df3-4dd5-93e0-013cd42546e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compile_model():\n",
    "    \"\"\"Creates, compiles and trains the model to predict woman and man images.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The model that will be trained to predict woman and man images.\n",
    "    \"\"\"\n",
    "\n",
    "    augmentation_layers = augment_model()\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # Note: the input shape is the desired size of the image: 150x150 with 3 bytes for color\n",
    "        tf.keras.layers.InputLayer(shape = (img_size, img_size, 3)),\n",
    "        augmentation_layers,\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        #####    CONV_LAYER_1:     #####\n",
    "        tf.keras.layers.Conv2D(n_filters_l1, (4, 4), activation = 'linear'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        #####    CONV_LAYER_2:     #####\n",
    "        tf.keras.layers.Conv2D(n_filters_l2, (3, 3), activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        #####    CONV_LAYER_3:     #####\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        #####    CONV_LAYER_4:     #####\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(dropout_value),\n",
    "        #####    BEFORE_LAST_LAYER:     #####\n",
    "        tf.keras.layers.Dense(n_units_last_layer, activation = 'relu'),\n",
    "        # It will contain a value from 0-1 where 0 for the class 'female' and 1 for the 'male'\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
    "\n",
    "    model.compile(\n",
    "        loss = loss,\n",
    "        optimizer = optimizer,\n",
    "        metrics = metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc4042-af60-4d6a-a989-f81761b5bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the compiled but untrained model\n",
    "model = create_and_compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abb112-31e4-462d-aeb5-ac878eb84cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks = [EarlyStoppingCallback()],\n",
    "    verbose = 2\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5a348-8f19-43d7-b96d-764b239959e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks=[early_stop],\n",
    "    verbose = 2\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ceea5-9f80-4011-aeb4-9951bc88ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks = [lr_callback],\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32643e-ba36-4ab6-bf3b-43313b52bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import load_model\n",
    "#model.save('gender_recognition_project04_v10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ce52e-7f7c-47c7-9d63-f8d21a4ef5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16344a-bbc7-4ed5-a01e-39b651f802c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history = pd.DataFrame(model.history.history)\n",
    "result_history.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8df04-5bbc-4d47-a131-1e4a3a1c7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history[['loss', 'val_loss']].plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b61e9e-deaf-4f91-8b90-d23082a18b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_history[['accuracy', 'val_accuracy']].plot(figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b259fcb-df40-4d21-a911-032373c25358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)\n",
    "print(model.evaluate(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce3add-a314-4d29-91ea-76cb3ba57cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = np.concatenate([y.numpy() for _, y in validation_dataset])\n",
    "y_pred_prob = model.predict(validation_dataset)\n",
    "# Convert probabilities to class labels (0:Female or 1:Male)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=['Female', 'Male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063526cc-e2a0-4c8c-b01c-47ae3654a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "img_size = img_size\n",
    "model = tf.keras.models.load_model(\"gender_recognition_project04_v10.h5\")\n",
    "\n",
    "# Load your personal image if you are interested to predict:\n",
    "your_image_path = \"D:\\\\Hossein's desktop files in Microsoft Studio Laptop\\\\Personal Photos\\\\Hossein_10.jpg\"\n",
    "\n",
    "img = load_img(your_image_path, target_size=(img_size, img_size))\n",
    "final_img = img_to_array(img)\n",
    "# Adding a batch dimension:\n",
    "final_img = np.expand_dims(final_img, axis=0)\n",
    "prediction = model.predict(final_img)\n",
    "result = \"Female\" if prediction > 0.5 else \"Male\"\n",
    "if result==\"Female\":\n",
    "    confidence = (model.predict(final_img)[0][0])*100\n",
    "else:\n",
    "    confidence = (1-model.predict(final_img)[0][0])*100\n",
    "print(f\"Prediction result: {result} (confidence= {confidence:.2f} %)\")\n",
    "\n",
    "# Visualize CNN Layers\n",
    "successive_feature_maps = visualization_model.predict(final_img)\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    if len(feature_map.shape) == 4:  # Only visualize conv/maxpool layers\n",
    "        n_features = feature_map.shape[-1]  # Number of filters\n",
    "        size = feature_map.shape[1]  # Feature map size\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "\n",
    "        for i in range(n_features):\n",
    "            x = feature_map[0, :, :, i]\n",
    "            x -= x.mean()\n",
    "            x /= (x.std() + 1e-8)  # Normalize\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')  # Convert to image format\n",
    "            display_grid[:, i * size: (i + 1) * size] = x\n",
    "\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='cividis')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199738b-2405-4223-9eae-21b5840224a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a87fb2-d828-420e-883f-709ca46ba4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb26553-bde6-4c23-ba84-69763691803d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
